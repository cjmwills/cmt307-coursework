{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/c1977808/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_pos, path_neg):\n",
    "    pos = pd.read_csv(path_pos, sep=\"\\n\", header=None, names=['review'])\n",
    "    pos['positive']=1\n",
    "    neg = pd.read_csv(path_neg, sep=\"\\n\", header=None, names=['review'])\n",
    "    neg['positive']=0\n",
    "    combined_df = pos.append(neg)\n",
    "    combined_df = shuffle(combined_df, random_state=42)\n",
    "    return(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data\n",
    "train = read_data(path_pos=\"Data/IMDb/train/imdb_train_pos.txt\",\n",
    "                  path_neg=\"Data/IMDb/train/imdb_train_neg.txt\")\n",
    "\n",
    "dev = read_data(path_pos=\"Data/IMDb/dev/imdb_dev_pos.txt\",\n",
    "                path_neg=\"Data/IMDb/dev/imdb_dev_neg.txt\")\n",
    "\n",
    "test = read_data(path_pos=\"Data/IMDb/test/imdb_test_pos.txt\",\n",
    "                 path_neg=\"Data/IMDb/test/imdb_test_neg.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>This is a comedy/romance movie directed by And...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>During the Sci-Fi TZ marathon of January 31, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>radio is possibly one of the best films i have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>I was -Unlike most of the reviewers- not born ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>When i started watching \"Surface\"for the first...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  positive\n",
       "4016  This is a comedy/romance movie directed by And...         0\n",
       "6475  During the Sci-Fi TZ marathon of January 31, 1...         1\n",
       "5684  radio is possibly one of the best films i have...         0\n",
       "862   I was -Unlike most of the reviewers- not born ...         1\n",
       "5970  When i started watching \"Surface\"for the first...         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive reviews\n",
      "-----\n",
      "7483\n",
      "\n",
      "No of negative reviews\n",
      "-----\n",
      "7517\n"
     ]
    }
   ],
   "source": [
    "print(\"No of positive reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[1])\n",
    "print(\"\\nNo of negative reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1 - tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature is based on a vocabulary of tokens collected from the training data. Term Frequency times Inverse Document Frequency will be used. Term Frequency (tf) counts the number of times a token in the vocabulary is used in each review, relative to how frequently it appears in that review. This acts as a way of normalising the count. Inverse Document Frequency (idf) penalises tokens that appear across many reviews, as these terms offer less information than those that appear in fewer reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create set of stopwords to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take set of stopwords from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# manually add more punctuation\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"'s\")\n",
    "stopwords.add(\"â€™\")\n",
    "stopwords.add(\"...\")\n",
    "stopwords.add(\"n't\")\n",
    "stopwords.add(\"'re\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\";\")\n",
    "stopwords.add(\"/\")\n",
    "stopwords.add(\">\")\n",
    "stopwords.add(\"<\")\n",
    "stopwords.add(\"br\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "stopwords.add(\"''\")\n",
    "stopwords.add(\"&\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a simple transormer so just the dataset can be used to feed into the full pipeline later. Currently, feature 1 pipeline takes `train['review']` while feature 2 pipeline takes `train` as it's argument. This needs to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selectReview(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return(X['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1_vocab = Pipeline([\n",
    "    ('select_review', selectReview()),\n",
    "    ('count', CountVectorizer(stop_words=stopwords, max_features=500)),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 2 - tf-idf (bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_vocab = Pipeline([\n",
    "    ('select_review', selectReview()),\n",
    "    ('count', CountVectorizer(stop_words=stopwords, max_features=500, ngram_range=(2,2))),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3 - Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getSentiment(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        features_array=[]\n",
    "        for index, row in X.iterrows():\n",
    "            pos = vader.polarity_scores(row['review'])['pos']\n",
    "            neu = vader.polarity_scores(row['review'])['neu']\n",
    "            neg = vader.polarity_scores(row['review'])['neg']\n",
    "            features_array.append([pos, neu, neg])\n",
    "        return(np.asarray(features_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3_sentiment = Pipeline([\n",
    "    ('get_sentiment', getSentiment())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = FeatureUnion(transformer_list=[\n",
    "    (\"feature_1_vocab\", feature_1_vocab),\n",
    "    (\"feature_2_vocab\", feature_2_vocab),\n",
    "    (\"feature_3_sentiment\", feature_3_sentiment)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_engineering.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1003)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and evaluate different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train['positive'])\n",
    "y_dev = np.asarray(dev['positive'])\n",
    "X_dev = feature_engineering.transform(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"svm_clf\", sklearn.svm.LinearSVC(loss='hinge'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c1977808/.local/share/virtualenvs/c1977808-tkY5RK3O/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)\n",
    "svm_clf_pred = svm_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.82      2482\n",
      "           1       0.80      0.89      0.84      2518\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.84      0.83      0.83      5000\n",
      "weighted avg       0.84      0.83      0.83      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, svm_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"svm_clf\", sklearn.svm.SVC(kernel=\"rbf\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm_clf.fit(X_train, y_train)\n",
    "rbf_svm_clf_pred = rbf_svm_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2482\n",
      "           1       0.85      0.85      0.85      2518\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, rbf_svm_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"svm_clf\", sklearn.svm.SVC(kernel=\"poly\", degree=3, coef0=1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm_clf.fit(X_train, y_train)\n",
    "poly_svm_clf_pred = poly_svm_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2482\n",
      "           1       0.85      0.85      0.85      2518\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, poly_svm_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "tree_pred = tree_clf.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.80      0.74      2482\n",
      "           1       0.77      0.64      0.69      2518\n",
      "\n",
      "    accuracy                           0.72      5000\n",
      "   macro avg       0.72      0.72      0.72      5000\n",
      "weighted avg       0.73      0.72      0.72      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, tree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `GridSearchCV` on shortlisted models to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'svm_clf__C': [0.1, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(svm_clf, param_grid, scoring='f1_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No of features: 10\n",
      "F-measure: 0.738\n",
      "\n",
      "No of features: 100\n",
      "F-measure: 0.815\n",
      "\n",
      "No of features: 500\n",
      "F-measure: 0.844\n",
      "\n",
      "No of features: 1000\n",
      "F-measure: 0.853\n"
     ]
    }
   ],
   "source": [
    "for i in [10,100,500,1000]:\n",
    "    selector = SelectKBest(chi2, k=i)\n",
    "    X_train_reduced = selector.fit_transform(X_train, np.asarray(train['positive']))\n",
    "    X_dev_reduced = selector.transform(X_dev)\n",
    "    rbf_svm_clf.fit(X_train_reduced, y_train)\n",
    "    rbf_svm_clf_pred = rbf_svm_clf.predict(X_dev_reduced)\n",
    "    f_measure = f1_score(y_dev, rbf_svm_clf_pred, average='weighted')\n",
    "    print('\\nNo of features: '+str(i))\n",
    "    print('F-measure: '+str(round(f_measure,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
