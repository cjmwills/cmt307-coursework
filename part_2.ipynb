{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import requests\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_pos, path_neg):\n",
    "    pos = pd.read_csv(path_pos, sep=\"\\n\", header=None, names=['review'])\n",
    "    pos['positive']=1\n",
    "    neg = pd.read_csv(path_neg, sep=\"\\n\", header=None, names=['review'])\n",
    "    neg['positive']=0\n",
    "    combined_df = pos.append(neg)\n",
    "    combined_df = shuffle(combined_df, random_state=42)\n",
    "    return(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data\n",
    "train = read_data(path_pos=\"Data/IMDb/train/imdb_train_pos.txt\",\n",
    "                  path_neg=\"Data/IMDb/train/imdb_train_neg.txt\")\n",
    "\n",
    "dev = read_data(path_pos=\"Data/IMDb/dev/imdb_dev_pos.txt\",\n",
    "                path_neg=\"Data/IMDb/dev/imdb_dev_neg.txt\")\n",
    "\n",
    "test = read_data(path_pos=\"Data/IMDb/test/imdb_test_pos.txt\",\n",
    "                 path_neg=\"Data/IMDb/test/imdb_test_neg.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive reviews\n",
      "-----\n",
      "7483\n",
      "\n",
      "No of negative reviews\n",
      "-----\n",
      "7517\n"
     ]
    }
   ],
   "source": [
    "print(\"No of positive reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[1])\n",
    "print(\"\\nNo of negative reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate lemmatizer, stopwords, and token retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a list of unique tokens that have been lemmatized and made lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "      list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "      for token in list_tokens_sentence:\n",
    "        list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, vocabulary):\n",
    "    features_array=[]\n",
    "    for index, row in df.iterrows():\n",
    "        tokens=get_tokens(row['review'])\n",
    "        features=np.zeros(len(vocabulary))\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in tokens:\n",
    "                features[i]=tokens.count(word)\n",
    "        features_array.append(features)\n",
    "    return np.asarray(features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create set of stopwords that will be removed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take set of stopwords from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# manually add more punctuation\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"'s\")\n",
    "stopwords.add(\"â€™\")\n",
    "stopwords.add(\"...\")\n",
    "stopwords.add(\"n't\")\n",
    "stopwords.add(\"'re\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\";\")\n",
    "stopwords.add(\"/\")\n",
    "stopwords.add(\">\")\n",
    "stopwords.add(\"<\")\n",
    "stopwords.add(\"br\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "stopwords.add(\"''\")\n",
    "stopwords.add(\"&\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getTokensVocab(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        list_tokens=[]\n",
    "        for index, row in X.iterrows():\n",
    "          sentence_split=nltk.tokenize.sent_tokenize(row['review'])\n",
    "          for sentence in sentence_split:\n",
    "            tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "            for token in tokens:\n",
    "              list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "        return(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sortTokens(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        dict_word_freq={}\n",
    "        for token in X:\n",
    "            if token in stopwords: continue\n",
    "            elif token not in dict_word_freq: dict_word_freq[token]=1\n",
    "            elseb: dict_word_freq[token]+=1\n",
    "        sorted_tokens = sorted(dict_word_freq.items(), key=lambda x: x[1], reverse=True)[:self.n]\n",
    "        return(sorted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getVocabulary(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        vocabulary=[]\n",
    "        for word,frequency in X:\n",
    "            vocabulary.append(word)\n",
    "        return(np.asarray(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, train):\n",
    "        self.train = train\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        x_train = get_features(self.train, X)\n",
    "        return(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1_vocab = Pipeline([\n",
    "    ('tokenize', getTokensVocab()),\n",
    "    ('sort', sortTokens(100)),\n",
    "    ('get_vocab', getVocabulary()),\n",
    "    ('get_features', getFeatures(train)),\n",
    "    ('minmax_scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06451613 0.04       0.11111111 ... 0.16666667 0.         0.        ]\n",
      " [0.         0.12       0.         ... 0.         0.         0.        ]\n",
      " [0.         0.04       0.11111111 ... 0.         0.         0.1       ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.3       ]\n",
      " [0.19354839 0.04       0.         ... 0.         0.         0.2       ]\n",
      " [0.06451613 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "f1 = feature_1_vocab.fit_transform(train)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
