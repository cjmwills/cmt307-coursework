{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/c1977808/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path_pos, path_neg):\n",
    "    pos = pd.read_csv(path_pos, sep=\"\\n\", header=None, names=['review'])\n",
    "    pos['positive']=1\n",
    "    neg = pd.read_csv(path_neg, sep=\"\\n\", header=None, names=['review'])\n",
    "    neg['positive']=0\n",
    "    combined_df = pos.append(neg)\n",
    "    combined_df = shuffle(combined_df, random_state=42)\n",
    "    return(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data\n",
    "train = read_data(path_pos=\"Data/IMDb/train/imdb_train_pos.txt\",\n",
    "                  path_neg=\"Data/IMDb/train/imdb_train_neg.txt\")\n",
    "\n",
    "dev = read_data(path_pos=\"Data/IMDb/dev/imdb_dev_pos.txt\",\n",
    "                path_neg=\"Data/IMDb/dev/imdb_dev_neg.txt\")\n",
    "\n",
    "test = read_data(path_pos=\"Data/IMDb/test/imdb_test_pos.txt\",\n",
    "                 path_neg=\"Data/IMDb/test/imdb_test_neg.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4016</th>\n",
       "      <td>This is a comedy/romance movie directed by And...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6475</th>\n",
       "      <td>During the Sci-Fi TZ marathon of January 31, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5684</th>\n",
       "      <td>radio is possibly one of the best films i have...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>I was -Unlike most of the reviewers- not born ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5970</th>\n",
       "      <td>When i started watching \"Surface\"for the first...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  positive\n",
       "4016  This is a comedy/romance movie directed by And...         0\n",
       "6475  During the Sci-Fi TZ marathon of January 31, 1...         1\n",
       "5684  radio is possibly one of the best films i have...         0\n",
       "862   I was -Unlike most of the reviewers- not born ...         1\n",
       "5970  When i started watching \"Surface\"for the first...         1"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of positive reviews\n",
      "-----\n",
      "7483\n",
      "\n",
      "No of negative reviews\n",
      "-----\n",
      "7517\n"
     ]
    }
   ],
   "source": [
    "print(\"No of positive reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[1])\n",
    "print(\"\\nNo of negative reviews\\n-----\")\n",
    "print(train['positive'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1 - tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first feature is based on a vocabulary of tokens collected from the training data. Term Frequency times Inverse Document Frequency will be used. Term Frequency (tf) counts the number of times a token in the vocabulary is used in each review, relative to how frequently it appears in that review. This acts as a way of normalising the count. Inverse Document Frequency (idf) penalises tokens that appear across many reviews, as these terms offer less information than those that appear in fewer reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create set of stopwords to remove later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take set of stopwords from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# manually add more punctuation\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"'s\")\n",
    "stopwords.add(\"â€™\")\n",
    "stopwords.add(\"...\")\n",
    "stopwords.add(\"n't\")\n",
    "stopwords.add(\"'re\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\";\")\n",
    "stopwords.add(\"/\")\n",
    "stopwords.add(\">\")\n",
    "stopwords.add(\"<\")\n",
    "stopwords.add(\"br\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "stopwords.add(\"''\")\n",
    "stopwords.add(\"&\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to create a simple transormer so just the dataset can be used to feed into the full pipeline later. Currently, feature 1 pipeline takes `train['review']` while feature 2 pipeline takes `train` as it's argument. This needs to be consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selectReview(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return(X['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1_vocab = Pipeline([\n",
    "    ('select_review', selectReview()),\n",
    "    ('count', CountVectorizer(stop_words=stopwords, max_features=500)),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 2 - tf-idf (bi-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_vocab = Pipeline([\n",
    "    ('select_review', selectReview()),\n",
    "    ('count', CountVectorizer(stop_words=stopwords, max_features=500, ngram_range=(2,2))),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 3 - Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getSentiment(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        features_array=[]\n",
    "        for index, row in X.iterrows():\n",
    "            pos = vader.polarity_scores(row['review'])['pos']\n",
    "            neu = vader.polarity_scores(row['review'])['neu']\n",
    "            neg = vader.polarity_scores(row['review'])['neg']\n",
    "            features_array.append([pos, neu, neg])\n",
    "        return(np.asarray(features_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3_sentiment = Pipeline([\n",
    "    ('get_sentiment', getSentiment())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering = FeatureUnion(transformer_list=[\n",
    "    (\"feature_1_vocab\", feature_1_vocab),\n",
    "    (\"feature_2_vocab\", feature_2_vocab),\n",
    "    (\"feature_3_sentiment\", feature_3_sentiment)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_train = feature_engineering.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 1003)\n"
     ]
    }
   ],
   "source": [
    "print(fe_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Currently unsure whether to perform feature selection as part of Pipeline or through a standard function. Pipeline offers auto tuning of parameters but function allows flexibility to specify test, dev, or train set to be used!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(matrix, data_split, k):\n",
    "    reduced_features = SelectKBest(chi2, k=k).fit_transform(matrix, np.asarray(data_split['positive']))\n",
    "    return(reduced_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_train = feature_selection(fe_train, train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO: include a justification for how many features to select**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and evaluate different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a series of different machine learning algorithms here such as; SVM (Linear, Polynomial and RBF), SGD, Decision Trees, Logistic Regression. Using GridSearch to test for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train['positive'])\n",
    "y_dev = np.asarray(dev['positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf_full = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_full.fit(fe_train,y_train)\n",
    "svm_clf_red = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf_red.fit(fs_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_dev = feature_engineering.transform(dev)\n",
    "fs_dev = feature_selection(fe_dev, dev, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predictions_full = svm_clf_full.predict(fe_dev)\n",
    "dev_predictions_red = svm_clf_red.predict(fs_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      2482\n",
      "           1       0.85      0.85      0.85      2518\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, dev_predictions_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      2482\n",
      "           1       0.71      0.81      0.76      2518\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.74      0.74      0.74      5000\n",
      "weighted avg       0.74      0.74      0.74      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_dev, dev_predictions_red))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEXT STEPS: run polynomial and RBF SVM using the kernel trick. Then run a decision tree and logistic regression to shortlist ~ five suitable models. Then run `GridSearchCV` to find the best combination of hyperparameters for each of the ~ five models. Run this inside a loop that performs feature selection on a . Finally, run the best performing model on the unseen test data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
