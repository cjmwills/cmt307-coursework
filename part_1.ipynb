{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "*What is the difference between a rule-based system and a machine learning\n",
    "system? (5%)*\n",
    "\n",
    "A rule-based system involves a set of pre-defined rules that are applied to data. The output of which are known as the answers. This system does not seek to learn from the input provided, it simply executes the rules defined. For example, if the height of a person is above 180cm label them 'tall', otherwise label them 'small'. \n",
    "\n",
    "A machine learning system is trained, rather than defined, by providing it with data and answers (only if it is supervised machine learning). The system looks for statistical structure in this input and outputs the rules to be used. For example, predict the height of school children in Cardiff by learning from data that provides the age and height of school children in Swansea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "*What is the difference between unsupervised and supervised learning? (5%)*\n",
    "\n",
    "Supervised learning is when a machine learning system is provided the output variable as well as the input variables. By providing the system with the 'answer' it is able learn the statistical characteristics of an observation that leads to a particular output (continuous or categorical). The resulting algorithm can then be used to predict future, unknown outputs. For example, predicting height given age by providing the height and age of school children.\n",
    "\n",
    "Unsupervised learning is when a machine learning system is only provided with input variables, no output variables are supplied. The system can't predict outcome as no 'answer' is supplied. Instead, it has to produce an algortihm to classify each observation to a pre-defined number of groups. To do this it looks for statistical similarities between the observations. For example, grouping animals that are similar given information about their size and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "*What do we mean when we say that a machine learning system is overfitting? (5%)*\n",
    "\n",
    "Overfitting occurs when a machine learning system produces an algorithm that too specifically fits itself to the training data provided. This will often result in high accuracy on that one dataset, but low accuracy when applied to new, unseen data. A system that is overfitting will try to over complicate the model and will not take into account the element of randomness (known as white noise) that occurs in each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical (85%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "*Your algorithm gets the following results in a classification experiment. Please compute the precision, recall, f-measure and accuracy manually (without the help of your computer/Python, please provide all steps and formulas). Include the process to get to the final result. (20%)*\n",
    "\n",
    "Id | Prediction | Gold\n",
    "- | - | -\n",
    "1 | True | True\n",
    "2 | True | True\n",
    "3 | False | True\n",
    "4 | True | True\n",
    "5 | False | True\n",
    "6 | False | True\n",
    "7 | True | True\n",
    "8 | True | True\n",
    "9 | True | True\n",
    "10 | False | False\n",
    "11 | False | False\n",
    "12 | False | False\n",
    "13 | True | False\n",
    "14 | False | False\n",
    "15 | False | False\n",
    "16 | False | False\n",
    "17 | False | False\n",
    "18 | True | False\n",
    "19 | True | False\n",
    "20 | False | False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the evaluation measures, we first need to construct the confusion matrix. This can be done by manually counting the combinations of True and False between the prediction and the gold standard i.e. True/True, True/False, False/True, False/False.\n",
    "\n",
    "Confusion Matrix | Predicted: True | Predicted: False | Total\n",
    "- | - | - | -\n",
    "Actual: True | 6 (TP) | 3 (FP) | 9\n",
    "Actual: False | 3 (FN) | 8 (TN) | 11\n",
    "Total | 9 | 11 | 20\n",
    "\n",
    "Where TP is the number of True Positives, FP is False Positives, FN is False Negatives, and TN is True Negatives. Now we can compute the following measures;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Defined as the proportion of individuals correctly classified.\n",
    "\n",
    "$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "$Accuracy = \\dfrac{6 + 8}{6 + 8 + 3 + 3}$\n",
    "\n",
    "$Accuracy = \\dfrac{14}{20}$\n",
    "\n",
    "$Accuracy = 0.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Defined as the proportion of positive predictions that are correct.\n",
    "\n",
    "$Precision = \\dfrac{TP}{TP + FP}$\n",
    "\n",
    "$Precision = \\dfrac{6}{6 + 3}$\n",
    "\n",
    "$Precision = \\dfrac{2}{3}$\n",
    "\n",
    "$Precision = 0.667$ $(3dp)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "Defined as the proportion of positive cases that were predicted as positive.\n",
    "\n",
    "$Recall = \\dfrac{TP}{TP + FN}$\n",
    "\n",
    "$Recall = \\dfrac{6}{6 + 3}$\n",
    "\n",
    "$Recall = \\dfrac{2}{3}$\n",
    "\n",
    "$Recall = 0.667$ $(3dp)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-Measure\n",
    "\n",
    "Defined as the harmonic mean of precision and recall. It's used to evaluate the performance of an algorithm in which you aren't neccesarily trying to improve only the precision or only the recall.\n",
    "\n",
    "$F_1 = 2 \\times \\dfrac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "$F_1 = 2 \\times \\dfrac{\\dfrac{2}{3} \\times \\dfrac{2}{3}}{\\dfrac{2}{3} + \\dfrac{2}{3}}$\n",
    "\n",
    "$F_1 = 2/3$\n",
    "\n",
    "$F_1 = 0.667$ $(3dp)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "*You are given a dataset (named Wine dataset) with different measured properties of different wines (dataset available in Learning Central). Your goal is to develop a machine learning model to predict the quality of an unseen wine given these properties. Train two machine learning regression models and check their performance. Write, for each of the models, the main Python instructions to train and predict the labels (one line each, no need to include any data preprocessing) and the performance in the test set in terms of Root Mean Squared Error (RMSE) (30%)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train = pd.read_csv(\"Data/Wine/wine_train.csv\", sep=';')\n",
    "wine_test = pd.read_csv(\"Data/Wine/wine_test.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "Least squares linear regression will be used to predict wine quality for the first model. It will use only one predictor variable; residual sugar. To begin, split the wine dataset by input variable (residual sugar) and output variable (wine quality). Repeat this with the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train_x_m1 = wine_train[['residual sugar']]\n",
    "wine_train_y = wine_train['quality']\n",
    "wine_test_x_m1 = wine_test[['residual sugar']]\n",
    "wine_test_y = wine_test['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by fitting a linear regression model to the training data. This will perform least squares regression which attempts to fit a model that minimises the sums of squares between the mapping function and the data. As the model is given the output variable (quality) this is a supervised machine learning model. Once trained, the `.predict` method can be called to predict wine quality of a new batch of wines, given their residual sugar levels. After this, root mean square error provides a measure of performance of the model and can be used to compare accuracy with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_m1 = LinearRegression()\n",
    "lin_reg_m1.fit(wine_train_x_m1, wine_train_y)\n",
    "predictions_m1 = lin_reg_m1.predict(wine_test_x_m1)\n",
    "rmse_m1 = sqrt(mean_squared_error(wine_test_y, predictions_m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Least squares linear regression will be used to predict wine quality for the second model. It will use two predictor variables; alcohol and volatile acidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_train_x_m2 = wine_train[['alcohol', 'volatile acidity']]\n",
    "wine_test_x_m2 = wine_test[['alcohol', 'volatile acidity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_m2 = LinearRegression()\n",
    "lin_reg_m2.fit(wine_train_x_m2, wine_train_y)\n",
    "predictions_m2 = lin_reg_m2.predict(wine_test_x_m2)\n",
    "rmse_m2 = sqrt(mean_squared_error(wine_test_y, predictions_m2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error for Model 1 \n",
      "----------\n",
      "0.886\n",
      "\n",
      " Root Mean Square Error for Model 2 \n",
      "----------\n",
      "0.788\n"
     ]
    }
   ],
   "source": [
    "print('Root Mean Square Error for Model 1 \\n----------')\n",
    "print(round(rmse_m1, 3))\n",
    "print('\\n Root Mean Square Error for Model 2 \\n----------')\n",
    "print(round(rmse_m2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Comparing the root mean square error (RMSE) of both models indicates that model 2 outperformed model 1. The root mean square error measures the difference between the predictions and the 'true' results. Therefore a smaller RMSE indicates a more accurate model. That is, by including both alcohol and volatile acidity in the model, better predictions of wine quality are achieved than if only residual sugar is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Train an SVM binary classifier using the Hateval dataset (available in Learning\n",
    "Central). The task consists of predicting whether a tweet represents hate speech\n",
    "or not. You can preprocess and choose the features freely. Evaluate the\n",
    "performance of your classifier in terms of accuracy using 10-fold cross-validation.\n",
    "Write a table with the results of the classifier (accuracy, precision, recall and\n",
    "F-measure) in each of the folds and write a small summary (up to 500 words) of\n",
    "how you preprocessed the data, chose the feature/s, and trained and evaluated\n",
    "your model (35%)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201</td>\n",
       "      <td>Hurray, saving us $$$ in so many ways @potus @...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202</td>\n",
       "      <td>Why would young fighting age men be the vast m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>203</td>\n",
       "      <td>@KamalaHarris Illegals Dump their Kids at the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204</td>\n",
       "      <td>NY Times: 'Nearly All White' States Pose 'an A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205</td>\n",
       "      <td>Orban in Brussels: European leaders are ignori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text  label\n",
       "0  201  Hurray, saving us $$$ in so many ways @potus @...      1\n",
       "1  202  Why would young fighting age men be the vast m...      1\n",
       "2  203  @KamalaHarris Illegals Dump their Kids at the ...      1\n",
       "3  204  NY Times: 'Nearly All White' States Pose 'an A...      0\n",
       "4  205  Orban in Brussels: European leaders are ignori...      0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateval = pd.read_csv(\"Data/Hateval/hateval.tsv\", delimiter='\\t')\n",
    "hateval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of tweets \n",
      "------\n",
      "9000\n",
      "\n",
      "Of which considered hate speech \n",
      "------\n",
      "3783\n"
     ]
    }
   ],
   "source": [
    "print(\"Total no of tweets \\n------\")\n",
    "print(hateval.label.count())\n",
    "print(\"\\nOf which considered hate speech \\n------\")\n",
    "print(hateval.label.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateval_train, hateval_test = train_test_split(hateval, train_size=0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 3)\n",
      "(1800, 3)\n"
     ]
    }
   ],
   "source": [
    "print(hateval_train.shape)\n",
    "print(hateval_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check proportions are similar between training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.577361\n",
       "1    0.422639\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateval_train['label'].value_counts() / len(hateval_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.588889\n",
       "1    0.411111\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hateval_test['label'].value_counts() / len(hateval_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of unique tokens that have been lemmatized and made lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_vocab(df):\n",
    "    list_tokens=[]\n",
    "    for index, row in df.iterrows():\n",
    "      sentence_split=nltk.tokenize.sent_tokenize(row['text'])\n",
    "      for sentence in sentence_split:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "          list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(string)\n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "      list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "      for token in list_tokens_sentence:\n",
    "        list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create set of stopwords that will be removed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take set of stopwords from nltk\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "# manually add more punctuation\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"#\")\n",
    "stopwords.add(\"@\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"'s\")\n",
    "stopwords.add(\"â€™\")\n",
    "stopwords.add(\"...\")\n",
    "stopwords.add(\"n't\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort tokens in dictionary to include the top n most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tokens(tokens, n):\n",
    "    dict_word_freq={}\n",
    "    for token in tokens:\n",
    "        if token in stopwords: continue\n",
    "        elif token not in dict_word_freq: dict_word_freq[token]=1\n",
    "        else: dict_word_freq[token]+=1\n",
    "    sorted_tokens = sorted(dict_word_freq.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return(sorted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the vocabulary to be used as features in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(sorted_tokens):\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_tokens:\n",
    "        vocabulary.append(word)\n",
    "    return(np.asarray(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run `hateval` data through functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "hateval_tokens = get_tokens_vocab(hateval_train)\n",
    "hateval_tokens_sorted = sort_tokens(hateval_tokens, 5)\n",
    "hateval_vocab = get_vocabulary(hateval_tokens_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return features as an array of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df, vocabulary):\n",
    "    features_array=[]\n",
    "    for index, row in df.iterrows():\n",
    "        tokens=get_tokens(row['text'])\n",
    "        features=np.zeros(len(vocabulary))\n",
    "        for i, word in enumerate(vocabulary):\n",
    "            if word in tokens:\n",
    "                features[i]=tokens.count(word)\n",
    "        features_array.append(features)\n",
    "    return np.asarray(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_features(hateval_train, hateval_vocab)\n",
    "x_test = get_features(hateval_test, hateval_vocab)\n",
    "y_train = np.asarray(hateval_train['label'])\n",
    "y_test = np.asarray(hateval_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predictions = svm_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[618 442]\n",
      " [199 541]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_test_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
